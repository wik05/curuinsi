{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "13f63dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72905ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new document camille\n",
    "rd = open(\"SOURCES/txt/raw_dictionary.txt\", \"r\")\n",
    "rd = rd.readlines()\n",
    "#rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "98934bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove lines with dic header content, pageline and title\n",
    "c_t = []\n",
    "for l in rd:\n",
    "    # Find pattern appearing on top of every pages, title, page number and words\n",
    "    t = re.search(r'TICUNA',l)\n",
    "    if t is None:\n",
    "        # remove new lines\n",
    "        l = l.strip()\n",
    "        c_t.append(l)\n",
    "\n",
    "\n",
    "# List of pos tag appearing in every entry, to be used in regex\n",
    "regex_dict_type = 'adj\\\\.|adv\\\\.|anim\\\\.|Antón\\\\.|art\\\\.|compl\\\\.|conj\\\\.|f\\\\.|fem\\\\.|fut\\\\.|inan\\\\.'\\\n",
    "                '|interj\\\\.|m\\\\.|masc\\\\.|pas\\\\.|pl\\\\.|posp\\\\.|pref\\\\. sust\\\\.|pref\\\\. verb\\\\.|prep\\\\.' \\\n",
    "                '|pres\\\\.|pron\\\\.|s\\\\.f\\\\.|sing\\\\.|Sinón\\\\.|s\\\\.m\\\\.|s\\\\.n\\\\.|suf\\\\. sust\\\\.|suf\\\\. verb\\\\.'\\\n",
    "                '|suj\\\\.|var\\\\.|v\\\\.e\\\\.|v\\\\.i\\\\.|v\\\\. recíp\\\\.|v\\\\. refl\\\\.|v\\\\.t\\\\.'\n",
    "\n",
    "entry = []\n",
    "temp = []\n",
    "for l in c_t:\n",
    "    has_pos =  re.search(r'%s'%regex_dict_type,l)\n",
    "    has_bulpo = re.search(r'•',l)\n",
    "    if has_pos:\n",
    "        if len(temp) == 0:\n",
    "            temp.append(l)\n",
    "            #print(temp)\n",
    "        else:\n",
    "            #print(temp)\n",
    "            e = ' '.join(temp)\n",
    "            entry.append(e)\n",
    "            temp = []\n",
    "            temp.append(l)\n",
    "    elif has_bulpo:\n",
    "        l = re.sub('•','',l)\n",
    "        if len(temp) == 0:\n",
    "            temp.append(l)\n",
    "            #print(temp)\n",
    "        else:\n",
    "            #print(temp)\n",
    "            e = ' '.join(temp)\n",
    "            entry.append(e)\n",
    "            temp = []\n",
    "            temp.append(l)\n",
    "        \n",
    "    else:\n",
    "        #print(temp)\n",
    "        temp.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31625ecf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "df = pd.DataFrame(entry)\n",
    "# df\n",
    "# Export the new cleaned text\n",
    "#with open('result.txt', 'a') as fp:\n",
    "#    fp.write(str(c_t))\n",
    "# Export as CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3e8cb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :clean code and create fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9840e6",
   "metadata": {},
   "source": [
    "### Files to load\n",
    "\n",
    "| Acronym      | Source | Langage     |\n",
    "| :---         |    :----:   |          ---: |\n",
    "|TS_DIC  |dictionnary | Tikuna-Spanish |\n",
    "|LLB_NEW |New Testament La Ligua Biblica |Tikuna|\n",
    "|LLB_OLD |Old Testament La Ligua Biblica |Tikuna|\n",
    "|T_Ebible|Ebible |Tikuna|\n",
    "|S_Ebible|EBible| Spanish|\n",
    "|T_OHCR  |Universal Declaration Of Human Rights | Tikuna|\n",
    "|S_OHCR  |Universal Declaration Of Human Rights  |Spanish|\n",
    "|T_CRU  |Crubadan Synsets  |Tikuna|\n",
    "\n",
    "* Ebible not converted in TXT for now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1622ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_dic = open(\"ts_dic.txt\", \"r\")\n",
    "#llb_new = open(\"llb_new.txt\", \"r\")\n",
    "#llb_old = open(\"llb_old.txt\", \"r\")\n",
    "#t_ohcr = open(\"t_ohcr.txt\", \"r\")\n",
    "#t_cru  = open('t_cru.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716d624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a4db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
