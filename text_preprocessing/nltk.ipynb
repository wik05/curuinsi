{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc974e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from pandas import option_context\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import spacy\n",
    "\n",
    "# Increase limit of characters processed\n",
    "# Need to specify to not use spcifics models to avoid overload\n",
    "# nlp_es.max_length = 2000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfa59b",
   "metadata": {},
   "source": [
    "## (fonctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "54b289dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct. import text, clean withite spaces, return cleaned text as list\n",
    "def clean_input(txt_file):\n",
    "    file_content = open(txt_path + file + \".txt\", \"r\").readlines()\n",
    "    file_cleaned = []\n",
    "    for line in file_content:\n",
    "        if not line.isspace():\n",
    "            file_cleaned.append(line.strip('\\n'))\n",
    "    file_cleaned = '-'.join(file_cleaned)\n",
    "    return(file_cleaned)\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean = re.sub('<.*>','',text)\n",
    "    return(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d26b9",
   "metadata": {},
   "source": [
    "## Textual corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4fd882c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : instead of length os text, transform into number of tokens and add csv\n",
    "# Create dictionary to store content\n",
    "corpus = {}\n",
    "# Define files names\n",
    "# TODO get from directory\n",
    "name_files_txt = ['llb_new','llb_old','t_ohcr','s_ohcr','t_cru']\n",
    "# explicit path\n",
    "# TODO clean name\n",
    "txt_path = 'sources/txt/'\n",
    "# Fill the doctionary with the clean content\n",
    "for file in name_files_txt:\n",
    "    corpus[file] = clean_input(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8844529",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'sources/txt/spa_ebible'\n",
    "spa_ebible = {}\n",
    "master_f = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            rename = re.search(r'([0-9]{3,3}.*[0-9]{2,2})', filename)\n",
    "            content = open(os.path.join(directory, filename), \"rt\",encoding=\"UTF-8-sig\").read()\n",
    "            content = re.sub('\\n','',content)  \n",
    "            master_f.append(content)          \n",
    "            spa_ebible[rename.group(0)] = content\n",
    "\n",
    "spa_ebible =  dict(sorted(spa_ebible.items()))\n",
    "\n",
    "directory = 'sources/txt/tik_ebible'\n",
    "tik_ebible = {}\n",
    "master_f = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            rename = re.search(r'([0-9]{3,3}.*[0-9]{2,2})', filename)\n",
    "            content = open(os.path.join(directory, filename), \"rt\",encoding=\"UTF-8-sig\").read()\n",
    "            content = re.sub('\\n','',content)\n",
    "            master_f.append(content)          \n",
    "            tik_ebible[rename.group(0)] = content\n",
    "\n",
    "tik_ebible =  dict(sorted(tik_ebible.items()))\n",
    "\n",
    "### HTML files, needs cleaning\n",
    "\n",
    "# directory = 'sources/original/tcaNT_html'\n",
    "# tik_ebible_full = {}\n",
    "\n",
    "# for filename in os.listdir(directory):\n",
    "#     f = os.path.join(directory,filename)\n",
    "#     if os.path.isfile(f):\n",
    "#         if filename.endswith(\".htm\"):\n",
    "#             output = pypandoc.convert_file(f, 'rst',format = 'html')\n",
    "#             filename = re.sub('\\.htm','',filename)\n",
    "#             tik_ebible_full[filename] = output\n",
    "\n",
    "# directory = 'sources/original/spavbl_html'\n",
    "# spa_ebible_full = {}\n",
    "\n",
    "# #spa_ebible_full\n",
    "# for key, value in spa_ebible_full.items():\n",
    "#     # Clean content by removing all tags and content from inside.\n",
    "#     # TODO : Keep hetml descritpion tags as meta data\n",
    "\n",
    "#     spa_ebible_full[key] = remove_html_tags(value)\n",
    "# # Sort dictionary to sort by apostle\n",
    "\n",
    "# spa_ebible_full =  dict(sorted(spa_ebible_full.items()))\n",
    "\n",
    "# for key, value in tik_ebible_full.items():\n",
    "#     tik_ebible_full[key] = remove_html_tags(value)\n",
    "#     tik_ebible_full =  dict(sorted(tik_ebible_full.items()))\n",
    "\n",
    "\n",
    "# def clean_annotate_ebible(chapter_txt):\n",
    "#     #char_to_remove = ['\\-','\\`','``','\\.','\\s*\\>\\s*','\\s*<\\s*','copyright','Â© 2008 WBT','_']\n",
    "#     container_type = ['mt',',main','m','q','p','s','chapterlabel']\n",
    "#     # from container :: [a-z]* to .., container annotation.\n",
    "#     c_chapter = re.sub('\\.\\. container\\:\\:','',chapter_txt)\n",
    "#     c_chapter = re.sub('\\n','',c_chapter)\n",
    "#     #c_chapter = re.search('^(.*?)[a-z]*','',chapter_txt)\n",
    "#     #c_chapter = re.search('(\\w+)',chapter_txt)\n",
    "#     #print(c_chapter.group(0))\n",
    "#     return(c_chapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fed81f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_content_bible(dic1,dic2):\n",
    "    bil_dict = {}\n",
    "    common_chapters = [i for i in dic1.keys() if i in dic2.keys()]\n",
    "    for element in common_chapters:\n",
    "        bil_dict[element] = {'spanish':'','tikuna':''}\n",
    "        bil_dict[element]['tikuna'] = dic1[element]\n",
    "        bil_dict[element]['spanish'] = dic2[element]\n",
    "    return(bil_dict)\n",
    "bil_dict = align_content_bible(tik_ebible,spa_ebible)\n",
    "# Remove first entry describing the corpus\n",
    "del bil_dict['000_000_000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "32eb7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_dict[1] = {'spanish':'','tikuna':''}\n",
    "bil_dict[1]['tikuna'] = corpus['t_ohcr']\n",
    "bil_dict[1]['spanish'] = corpus['s_ohcr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7af3a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dic = pd.read_csv(r'../dic_preprocessing/output.csv')\n",
    "ts_dic = ts_dic[ts_dic['example_spanish'].notna()].reset_index(drop=True)\n",
    "ex_t = ts_dic['example_tikuna']\n",
    "ex_s = ts_dic['example_spanish']\n",
    "# Keep the index of the source df ?\n",
    "def align_examples(tikuna,spanish):\n",
    "    examplen = {}\n",
    "    for i in range(len(tikuna)):\n",
    "        examplen[i] = {'spanish':'','tikuna':''}\n",
    "        examplen[i]['tikuna'] = tikuna[i]\n",
    "        examplen[i]['spanish'] = spanish[i]\n",
    "    return(examplen)\n",
    "dic_examples = align_examples(ex_t,ex_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c492bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_output_training = {'ebible':'','ohcr':'','examples':''}\n",
    "master_output_training['ebible'] = bil_dict\n",
    "master_output_training['ohcr'] = bil_dict['ohcr']\n",
    "master_output_training['examples'] = dic_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fd121",
   "metadata": {},
   "source": [
    "# Master Content Database \n",
    "\n",
    "The data is store in a dictionnary here is an small overview of the data\n",
    "\n",
    "| Key    | Subkeys      |   Value   | Content   ||\n",
    "| :---   |    :----:    |   ---:    |---:       |--:|\n",
    "|TS_DIC  | Column names |   Tokens  | translations||\n",
    "|LLB_NEW | Chapter Name |   Tikuna  |||\n",
    "|LLB_OLD | Chapter Name |   Tikuna  |||\n",
    "|T_Ebible| Chapter Name |   Tikuna  |||\n",
    "|S_Ebible| Chapter Name |   Spanish |||\n",
    "|T_OHCR  | Alinea       |   Tikuna  |||\n",
    "|S_OHCR  | Alinea       |   Spanish |||\n",
    "|T_CRU   | None         |   Tikuna  |||\n",
    "\n",
    "*TODO : Draw the data structure tree. What has to be kept in hierachical order, what part pas whole text ?*\n",
    "*Do both for modularity*\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5259c7e28202d7baa197c6712b7a302abbfc3c938ac1d9f5026d525f7df66357"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
