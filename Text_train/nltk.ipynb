{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4bc974e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import spacy\n",
    "import pypandoc\n",
    "\n",
    "# increase limit of characters processed\n",
    "# need to specify to not use spcifics models to avoid overload\n",
    "#nlp_es.max_length = 2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abfa59b",
   "metadata": {},
   "source": [
    "## (fonctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "54b289dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct. import text, clean withite spaces, return cleaned text as list\n",
    "def clean_input(txt_file):\n",
    "    file_content = open(txt_path + file + \".txt\", \"r\").readlines()\n",
    "    file_cleaned = []\n",
    "    for line in file_content:\n",
    "        if not line.isspace():\n",
    "            file_cleaned.append(line.strip('\\n'))\n",
    "    return(file_cleaned)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d26b9",
   "metadata": {},
   "source": [
    "## data description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb85cc",
   "metadata": {},
   "source": [
    "\n",
    "| Acronym      | Source | Langage     |\n",
    "| :---         |    :----:   |          ---: |\n",
    "|TS_DIC  |dictionnary | Tikuna-Spanish |\n",
    "|LLB_NEW |New Testament La Ligua Biblica |Tikuna|\n",
    "|LLB_OLD |Old Testament La Ligua Biblica |Tikuna|\n",
    "|T_Ebible|Ebible |Tikuna|\n",
    "|S_Ebible|EBible| Spanish|\n",
    "|T_OHCR  |Universal Declaration Of Human Rights | Tikuna|\n",
    "|S_OHCR  |Universal Declaration Of Human Rights  |Spanish|\n",
    "|T_CRU  |Crubadan Synsets  |Tikuna|\n",
    "|T_CANT  |readaloud bible parts  |Tikuna|\n",
    "\n",
    "- Ebible not converted in TXT for now\n",
    "- Missing Spanish llb bible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5296a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary with all the content\n",
    "content = {}\n",
    "# import text files\n",
    "name_files_txt = ['llb_new','llb_old','t_ohcr','s_ohcr','t_cru']\n",
    "txt_path = 'SOURCES/txt/'\n",
    "for file in name_files_txt :\n",
    "    file_content = open(txt_path + file + \".txt\", \"r\").readlines()\n",
    "    file_cleaned = []\n",
    "    for line in file_content:\n",
    "        if not line.isspace():\n",
    "            file_cleaned.append(line.strip('\\n'))\n",
    "    content[file] = file_cleaned\n",
    "\n",
    "\n",
    "# TODO : instead of length os text, transform into number of tokens and add csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4fd882c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store content\n",
    "content2 = {}\n",
    "# Define files names\n",
    "# TODO get from directory\n",
    "name_files_txt = ['llb_new','llb_old','t_ohcr','s_ohcr','t_cru']\n",
    "# explicit path\n",
    "# TODO clean name\n",
    "txt_path = 'SOURCES/txt/'\n",
    "# Fill the doctionary with the clean content\n",
    "for file in name_files_txt:\n",
    "    content2[file] = clean_input(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "db5e3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean to get only functions\n",
    "def fill_dic(file_list):\n",
    "    dic_content = {}\n",
    "    for file in file_list:\n",
    "        dic_content[file] = clean_input(file)\n",
    "    return(dic_content)\n",
    "\n",
    "s = fill_dic(name_files_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "eaa8b10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llb_new</th>\n",
       "      <th>llb_old</th>\n",
       "      <th>t_ohcr</th>\n",
       "      <th>s_ohcr</th>\n",
       "      <th>t_cru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>293437</td>\n",
       "      <td>276768</td>\n",
       "      <td>2582</td>\n",
       "      <td>1888</td>\n",
       "      <td>3035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   llb_new  llb_old  t_ohcr  s_ohcr  t_cru\n",
       "0   293437   276768    2582    1888   3035"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Len of text corpus\n",
    "token_size_texts = {}\n",
    "for key,value in content2.items():\n",
    "    text = ''.join(value)\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    tok_words = tokenizer.tokenize(text)\n",
    "    n_tokens = len(tok_words)\n",
    "    token_size_texts[key] = n_tokens\n",
    "token_size_texts = pd.DataFrame(token_size_texts, index=[0])\n",
    "token_size_texts\n",
    "# Remove punctuation to have only words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa55b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44265bb4",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "686132fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size of text files\n",
    "length_dict = {key: len(value) for key, value in content.items()}\n",
    "df_size = pd.DataFrame(length_dict.items(),columns=['name','size'])\n",
    "df_size = pd.pivot_table(df_size,index='name')\n",
    "# plot_size = df_size.plot(kind='bar');\n",
    "# TODO : adapt axis, sort sizes, legends, title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc05640",
   "metadata": {},
   "source": [
    "### Tikuna - Spanish dictionnary\n",
    "#### todo:\n",
    "- rename columns\n",
    "- table token_pos\n",
    "- token_spa list > spacy tag\n",
    "- compare ?\n",
    "- Create new columns\n",
    "- examples spa, exemple tik\n",
    "- pos tag examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "a03897ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b583fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new output from Anderson dictionary\n",
    "ts_dic = pd.read_csv('SOURCES/csv/ts_dic.csv')\n",
    "# ts_dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8928a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = []\n",
    "for i in range(ts_dic.shape[0]):\n",
    "    str = ts_dic['dic_entry'][i]\n",
    "    match = re.search(r'([A-Z].*\\.)',str)\n",
    "    if match:\n",
    "        str2 = match.group(0)\n",
    "        match2 = re.search(r'[\\(]',str2)\n",
    "        if match2:\n",
    "            example.append('')\n",
    "        else:\n",
    "            example.append(match.group(0))\n",
    "    else:\n",
    "        example.append('')\n",
    "    # * cases, tikuna esp\n",
    "    # mismatch an entry without pos tag\n",
    "    # Spanish descritpion of meaning\n",
    "ts_dic['example'] = example\n",
    "\n",
    "exclude = [None,'',' ','Var.','\\n','\\t','\\r','\\f']\n",
    "s = 0\n",
    "for i in range(ts_dic.shape[0]):\n",
    "    str = ts_dic['example'][i]\n",
    "    match = re.search(r'(?P<tikuna>[A-Z].*(\\.|\\!|\\?|\\¿))\\s*'\n",
    "    '(?P<spanish>[A-ZÈÉ].*(\\.|\\!|\\?|\\¿))',str)\n",
    "    if match:\n",
    "        #print(match.group(1))\n",
    "        pass\n",
    "    else:\n",
    "        if ts_dic['example'][i] not in exclude:\n",
    "            str = ts_dic['example'][i]\n",
    "            match = re.search(r'[A-Z].*\\.?\\s?[A-Z].*\\.?',str)\n",
    "            if match:\n",
    "                #print(ts_dic['example'][i])\n",
    "                match2 = re.search(r'(Indica|Se usa|\\[])',str)\n",
    "                if match2:\n",
    "                    pass\n",
    "                else:\n",
    "                    ts_dic['example'][i] = re.sub(r'(\\s+([A-Z]))',r'\\.\\1',str)\n",
    "            else:\n",
    "                #print(ts_dic['example'][i])\n",
    "                s += 1\n",
    "                pass\n",
    "print(s)\n",
    "# in the 220 examples, still good example with tikuna and spanish\n",
    "# Spanish begins with lowe case, difficult match and add point.\n",
    "# Verify number of line and clean manually or check with a spanish dict.\n",
    "# Embed this in a fonction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive data on the dictinonary\n",
    "#ts_dic\n",
    "# Count : number of exmaples, number of missing spanish (def)\n",
    "# Look for \"missing occurwnce acroos all the dataset\"\n",
    "# Longueur des exemmpes\n",
    "# Compter le nombre de mot unique en esp\n",
    "# how many spanis word appearing many time in entries\n",
    "# install the python wrapper, check how many occurences already in the spanish worndet\n",
    "#ts_dic['token_pos'].value_counts()\n",
    "# Split list of pos for categories\n",
    "\n",
    "#ts_dic['token_spa'].isnull().sum()\n",
    "#len(ts_dic[ts_dic['example'] != '']) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56378741",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebible text processing\n",
    "# import files\n",
    "# sum in one\n",
    "# dictionary with file name numbers and content\n",
    "# find pattern between spanish and ticuna in names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf5347",
   "metadata": {},
   "source": [
    "Spanish tikuna bibles\n",
    "LLB : find selected tikuna and match part with spanigh\n",
    "ebible:\n",
    "all in one text\n",
    "align put in single dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f8844529",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'SOURCES/txt/spa_ebible'\n",
    "spa_ebible = {}\n",
    "master_f = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            rename = re.search(r'([0-9]{3,3}.*[0-9]{2,2})', filename)\n",
    "            content = open(os.path.join(directory, filename), \"rt\").read()  \n",
    "            master_f.append(content)          \n",
    "            spa_ebible[rename.group(0)] = content\n",
    "\n",
    "spa_ebible =  dict(sorted(spa_ebible.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "52ea8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'SOURCES/txt/tik_ebible'\n",
    "tik_ebible = {}\n",
    "master_f = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            rename = re.search(r'([0-9]{3,3}.*[0-9]{2,2})', filename)\n",
    "            content = open(os.path.join(directory, filename), \"rt\",encoding=\"UTF-8-sig\").read()\n",
    "            content = re.sub('\\n','',content)\n",
    "            master_f.append(content)          \n",
    "            tik_ebible[rename.group(0)] = content\n",
    "\n",
    "tik_ebible =  dict(sorted(tik_ebible.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b944621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'SOURCES/original/tcaNT_html'\n",
    "tik_ebible_full = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename.endswith(\".htm\"):\n",
    "            output = pypandoc.convert_file(f, 'rst',format = 'html')\n",
    "            filename = re.sub('\\.htm','',filename)\n",
    "            tik_ebible_full[filename] = output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "8aa0be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'SOURCES/original/spavbl_html'\n",
    "spa_ebible_full = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory,filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename.endswith(\".htm\"):\n",
    "            output = pypandoc.convert_file(f, 'rst',format = 'html')\n",
    "            filename = re.sub('\\.htm','',filename)\n",
    "            spa_ebible_full[filename] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9fd121",
   "metadata": {},
   "source": [
    "# compare loading html and cleaning with \n",
    "pypandoc.convert_file('chapters/*.md', 'docx', outputfile=\"somefile.docx\")\n",
    "and create text files\n",
    "then match names of texts in tikuna and spanish\n",
    "create a graphical map in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "20152993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyandoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7117066",
   "metadata": {},
   "source": [
    "##  spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b316682",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4276e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load large spacy models\n",
    "nlp_es = spacy.load(\"es_dep_news_trf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260e6ee",
   "metadata": {},
   "source": [
    "### pos tagging texts in spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "387b8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tag function. list as input, list of tupple as output.\n",
    "def tag_text(text_list):\n",
    "    tag_list = []\n",
    "    s = ''.join([str(line) for line in text_list])\n",
    "    doc = nlp_es(s,disable=['ner'])\n",
    "    for token in doc:\n",
    "        token_tupple = (token.text, token.pos_, token.dep_)\n",
    "        tag_list.append(token_tupple)\n",
    "    return(tag_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e108752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "05bbf266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1df3c8e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca16a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annoter les données en html espagnol - tikuna pour aligner\n",
    "# txt, paragraphe, phrase, token\n",
    "# Delta Crubadan & Dataset\n",
    "# Match dictionnary content with bible\n",
    "# Create function for matching efficient"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5259c7e28202d7baa197c6712b7a302abbfc3c938ac1d9f5026d525f7df66357"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
